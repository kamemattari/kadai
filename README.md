## 概要
本リポジトリでは、2層ニューラルネットワークをNumPyのみを用いて
隠れ層を1層もつ2層ニューラルネットワークを実装し、
線形分離不可能なXOR問題の学習を行った。

XOR問題は、入力と出力の関係が線形分離できないため、
1層のニューラルネットワークでは解くことができない。
そのため、隠れ層を持つ2層構造の必要性を確認する題材として広く用いられる。

## XOR問題について
XOR（排他的論理和）問題は、2入力1出力の分類問題であり、
以下の対応関係を持つ。

| x1 | x2 | y |
|----|----|---|
| 0  | 0  | 0 |
| 0  | 1  | 1 |
| 1  | 0  | 1 |
| 1  | 1  | 0 |

この問題は入力空間上で直線によって分離できないため、隠れ層を持たない線形モデルでは解くことができない。

## 2層ニューラルネットワークの構造

本実装では、入力層・隠れ層・出力層からなる
全結合の2層ニューラルネットワークを用いる。

入力層は2ユニットからなり、
XOR問題の2次元入力 $(x_1, x_2)$ を受け取る。
隠れ層は2ユニットで構成され、
入力層のすべてのユニットと結合している。
出力層は1ユニットで構成され、
隠れ層の出力をもとに最終的な予測値を出力する。

各層では、重み付き和とバイアスを計算した後、
活性化関数による非線形変換を行う。


## 活性化関数

本実装では、隠れ層および出力層の活性化関数として
シグモイド関数を用いる。

シグモイド関数は以下で定義される。

$$
\sigma(x) = \frac{1}{1 + e^{-x}}
$$

実装では、順伝播時に得られた出力値を用いて
この微分を計算している。

## 順伝播

順伝播では、入力から出力までの計算を前向きに行う。

入力行列を $\mathbf{X}$ とすると、
隠れ層の出力 $\mathbf{h}$ は次式で計算される。

$$
\mathbf{h} = \sigma(\mathbf{X}\mathbf{W}_{hidden} + \mathbf{b}_{hidden})
$$

次に、隠れ層の出力を用いて、
出力層の出力 $\hat{\mathbf{y}}$ を計算する。

$$
\hat{\mathbf{y}} = \sigma(\mathbf{h}\mathbf{W}_{output} + \mathbf{b}_{output})
$$

## 誤差関数

ネットワークの出力 $\hat{\mathbf{y}}$ と
正解ラベル $\mathbf{y}$ の差を誤差として定義する。

$$
\mathbf{error} = \mathbf{y} - \hat{\mathbf{y}}
$$

この誤差をもとに、
誤差逆伝播法によって各パラメータを更新する。

## 誤差逆伝播

誤差逆伝播法では、出力層から入力層に向かって
誤差を伝播させることで、各層の勾配を計算する。

出力層のデルタは以下のように計算される。

$$
\delta_{output}
= (\mathbf{y} - \hat{\mathbf{y}}) \cdot \sigma'(\hat{\mathbf{y}})
$$

次に、隠れ層のデルタを計算する。

$$
\delta_{hidden}
= (\delta_{output}\mathbf{W}_{output}^T)
\cdot \sigma'(\mathbf{h})
$$

## パラメータ更新

学習率を $\eta$ とすると、
各パラメータは以下の式に従って更新される。

$$
\mathbf{W}_{output}
\leftarrow
\mathbf{W}_{output} + \eta \mathbf{h}^T \delta_{output}
$$

$$
\mathbf{b}_{output}
\leftarrow
\mathbf{b}_{output} + \eta \sum \delta_{output}
$$

$$
\mathbf{W}_{hidden}
\leftarrow
\mathbf{W}_{hidden} + \eta \mathbf{X}^T \delta_{hidden}
$$

$$
\mathbf{b}_{hidden}
\leftarrow
\mathbf{b}_{hidden} + \eta \sum \delta_{hidden}
$$

## 結果

学習回数（エポック数）を変化させた場合の出力結果を以下に示す。
各値は、入力 $(x_1, x_2)$ に対するネットワークの出力 $\hat{y}$ を表している。

### 学習回数と出力結果の比較

| 学習回数 | (0,0) | (0,1) | (1,0) | (1,1) |
|----------|-------|-------|-------|-------|
| 1000     | 0.248 | 0.500 | 0.723 | 0.558 |
| 5000     | 0.063 | 0.498 | 0.920 | 0.511 |
| 10000    | 0.038 | 0.499 | 0.952 | 0.504 |

### 考察

学習回数が少ない場合（1000回）では、
XOR問題に対する出力が中間的な値となっており、
正しい分類を十分に行えていないことが分かる。

一方で、学習回数を増加させるにつれて、
$(0,0)$ および $(1,1)$ に対する出力は 0 に近づき、
$(0,1)$ および $(1,0)$ に対する出力は 1 に近づいている。

このことから、反復回数を増やすことで
2層ニューラルネットワークが
線形分離不可能な XOR 問題を正しく学習できることが確認できた。
