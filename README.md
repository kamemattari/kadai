# XOR問題を解く2層ニューラルネットワークの実装（NumPy）

## 概要
本リポジトリでは、NumPyのみを用いて
隠れ層を1層もつ2層ニューラルネットワークを実装し、
線形分離不可能なXOR問題の学習を行った。

XOR問題は、入力と出力の関係が線形分離できないため、
隠れ層を持たない1層のニューラルネットワークでは解くことができない。
そのため、隠れ層を持つ2層構造の必要性を確認する題材として広く用いられる。

## XOR問題について
XOR（排他的論理和）問題は、2入力1出力の分類問題であり、
以下の対応関係を持つ。

| x1 | x2 | y |
|----|----|---|
| 0  | 0  | 0 |
| 0  | 1  | 1 |
| 1  | 0  | 1 |
| 1  | 1  | 0 |

この問題は入力空間上で直線によって分離できないため、隠れ層を持たない線形モデルでは解くことができない。

## 2層ニューラルネットワークの構造

本実装では、入力層・隠れ層・出力層からなる
全結合の2層ニューラルネットワークを用いる。

- 入力層：2ユニット（ $ x_1, x_2 $ ）
- 隠れ層：2ユニット
- 出力層：1ユニット

各層では、重み付き和とバイアスを計算した後、
活性化関数による非線形変換を行う。


## 活性化関数

隠れ層および出力層の活性化関数として
シグモイド関数を用いる。

$$
\sigma(x) = \frac{1}{1 + e^{-x}}
$$

シグモイド関数の微分は以下で与えられる。

$$
\sigma'(x) = \sigma(x)(1-\sigma(x))
$$

## 損失関数

ネットワークの出力 $\hat{y}$ と正解ラベル $y$ の差を評価するため、
二乗誤差を損失関数として用いる。

$$
L = \frac{1}{2} \sum_i (y_i - \hat{y}_i)^2
$$

二乗誤差は出力と正解の差を連続値として評価できる。
本課題ではネットワーク構造および誤差逆伝播の理解を目的としており、
問題規模も小さいため、損失関数として二乗誤差を採用した。

なお、分類問題ではクロスエントロピー損失が用いられることが多いが、
本実装では数式の単純さを優先して二乗誤差を用いている。

## 順伝播

入力行列を $\mathbf{X}$ とすると、
隠れ層の出力 $\mathbf{h}$ は次式で計算される。

$$
\mathbf{h} = \sigma(\mathbf{X}\mathbf{W}_{hidden} + \mathbf{b}_{hidden})
$$

次に、隠れ層の出力を用いて、
出力層の出力 $\hat{\mathbf{y}}$ を計算する。

$$
\hat{\mathbf{y}} = \sigma(\mathbf{h}\mathbf{W}_{output} + \mathbf{b}_{output})
$$

## 誤差逆伝播法

誤差逆伝播法では、損失関数 $L$ を各パラメータで偏微分し、
その勾配を用いて重みを更新する。

計算を簡潔にするため、
出力と正解の差を補助変数として

$$
\mathbf{error} = \mathbf{y} - \hat{\mathbf{y}}
$$

と定義する。

### 出力層

出力層の重み $W_{output}$ に対する勾配は、
チェインルールを用いて以下のように分解できる。

$$
\frac{\partial L}{\partial W_{output}}
=\frac{\partial L}{\partial \hat{y}}
\frac{\partial \hat{y}}{\partial z_{output}}
\frac{\partial z_{output}}{\partial W_{output}}
$$

ここで、

$$
z_{output} = \mathbf{h} W_{output} + b_{output}, \quad
\hat{y} = \sigma(z_{output})
$$

である。

これより、出力層の誤差項（デルタ）は

$$
\delta_{output}
= \mathbf{error} \cdot \sigma'(\hat{y})
$$

として計算される。

### 隠れ層

隠れ層では、出力層から伝播してきた誤差を用いて、

$$
\delta_{hidden}
= (\delta_{output} \mathbf{W}_{output}^T)
\cdot \sigma'(\mathbf{h})
$$

と計算される。

## パラメータ更新

学習率を $\eta$ とすると、
各パラメータは以下の式に従って更新される。

$$
\mathbf{W}_{output}
\leftarrow
\mathbf{W}_{output} + \eta \mathbf{h}^T \delta_{output}
$$

$$
\mathbf{b}_{output}
\leftarrow
\mathbf{b}_{output} + \eta \sum \delta_{output}
$$

$$
\mathbf{W}_{hidden}
\leftarrow
\mathbf{W}_{hidden} + \eta \mathbf{X}^T \delta_{hidden}
$$

$$
\mathbf{b}_{hidden}
\leftarrow
\mathbf{b}_{hidden} + \eta \sum \delta_{hidden}
$$

## 実行方法

本リポジトリは GitHub からクローンして実行できる。

```bash
git clone https://github.com/kamemattari/kadai.git
cd kadai
python two_layer_xor.py

```

## 動作環境

本実装は以下の環境で動作確認を行った。
- Python 3.10

- NumPy 1.26.0

## 結果

学習回数（エポック数）を変化させた場合の出力結果を以下に示す。
各値は、入力 $(x_1, x_2)$ に対するネットワークの出力 $\hat{y}$ を表している。

### 学習回数と出力結果の比較

学習回数を変化させた場合の出力結果を以下に示す。

| 学習回数 | (0,0) | (0,1) | (1,0) | (1,1) |
|----------|-------|-------|-------|-------|
| 1000     | 0.248 | 0.500 | 0.723 | 0.558 |
| 5000     | 0.063 | 0.498 | 0.920 | 0.511 |
| 10000    | 0.038 | 0.499 | 0.952 | 0.504 |

## 考察

学習回数が少ない場合（1000回）では、
XOR問題に対する出力が中間的な値となっており、
正しい分類を十分に行えていないことが分かる。

一方で、学習回数を増加させるにつれて、
$(0,0)$ および $(1,1)$ に対する出力は 0 に近づき、
$(0,1)$ および $(1,0)$ に対する出力は 1 に近づいている。

このことから、反復回数を増やすことで
2層ニューラルネットワークが
線形分離不可能な XOR 問題を正しく学習できることが確認できた。

なお、本実装では学習の収束を安定して確認するため、
比較的多めの学習回数（最大10000回）を設定している。
XOR問題は初期重みによって収束の速さが変わるため、
学習のばらつきを避ける目的で十分な反復回数を用いた。

また、XOR問題は0/1の分類問題であるが、
誤差逆伝播法を用いた学習を行うため、
活性化関数には微分可能なシグモイド関数を採用している。
ステップ関数は微分不可能であるため、
勾配に基づく学習には適さない。

## 参考文献

- [ニューラルネットワークを自作してみる（XOR） | Qiita](https://qiita.com/omiita/items/1735c1d048fe5f611f80)  
  誤差逆伝播法の数式展開を参考にした。

- [深層学習の基礎（XOR） | deepage](https://deepage.net/deep_learning/2016/10/04/xor_problem.html)  
  XOR問題の設定および図を参考にした。

- [Neural Network in Python | mlnotebook](https://mlnotebook.github.io/post/nn-in-python/)  
  NumPyによる実装構成を参考にした。

## ライセンス

- このソフトウェアパッケージは，3条項BSDライセンスの下，再頒布および使用が許可されます．

- © 2025–2026 Aika Katsuki
